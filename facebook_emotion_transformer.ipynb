{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3238154,"sourceType":"datasetVersion","datasetId":1962861}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"205e5536","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport re\nimport math\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:33:49.747212Z","iopub.execute_input":"2025-10-15T19:33:49.747477Z","iopub.status.idle":"2025-10-15T19:33:49.824762Z","shell.execute_reply.started":"2025-10-15T19:33:49.747459Z","shell.execute_reply":"2025-10-15T19:33:49.823942Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":51},{"id":"e319e06a","cell_type":"code","source":"DATA_PERCENTAGE = 100\n\ndf = pd.read_csv(\"/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\")\n\ndef normalize_text(text):\n    if pd.isnull(text): return \"\"\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text.strip())\n    text = re.sub(r'\\s([?.!,;:])', r'\\1', text)\n    text = re.sub(r'([?.!,;:])(?=\\w)', r'\\1 ', text)\n    return text\n\nfor col in ['Situation', 'emotion', 'empathetic_dialogues', 'labels']:\n    df[col] = df[col].apply(normalize_text)\n\nemotion_counts = df['emotion'].value_counts()\nvalid_emotions = emotion_counts[emotion_counts >= 50].index\ndf = df[df['emotion'].isin(valid_emotions)]\n\nif DATA_PERCENTAGE < 100:\n    df = df.sample(frac=DATA_PERCENTAGE/100, random_state=42).reset_index(drop=True)\n    print(f'Using {DATA_PERCENTAGE}% of data: {len(df)} samples')\n\nfrom sklearn.model_selection import train_test_split\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:33:49.826135Z","iopub.execute_input":"2025-10-15T19:33:49.826307Z","iopub.status.idle":"2025-10-15T19:33:52.509528Z","shell.execute_reply.started":"2025-10-15T19:33:49.826293Z","shell.execute_reply":"2025-10-15T19:33:52.508798Z"}},"outputs":[{"name":"stdout","text":"Train: 51672, Val: 6459, Test: 6460\n","output_type":"stream"}],"execution_count":52},{"id":"49681f7f","cell_type":"code","source":"train_text = train_df[['Situation', 'emotion', 'empathetic_dialogues', 'labels']].fillna('').agg(' '.join, axis=1)\ntokens = []\nfor text in train_text:\n    tokens.extend(word_tokenize(text))\n\ncounter = Counter(tokens)\nSPECIAL = ['<pad>', '<bos>', '<eos>', '<unk>', '<sep>']\nEMOTIONS = [f'<emotion_{e}>' for e in sorted(train_df['emotion'].unique())]\nvocab_list = SPECIAL + EMOTIONS + [w for w, _ in counter.most_common()]\nvocab = {w: i for i, w in enumerate(vocab_list)}\ninv_vocab = {i: w for w, i in vocab.items()}\n\ndef encode(text, add_bos_eos=True):\n    tokens = word_tokenize(text)\n    if add_bos_eos:\n        tokens = ['<bos>'] + tokens + ['<eos>']\n    return [vocab.get(t, vocab['<unk>']) for t in tokens]\n\ndef decode(ids, remove_special=True):\n    words = [inv_vocab.get(i, '<unk>') for i in ids]\n    if remove_special:\n        words = [w for w in words if w not in SPECIAL and not w.startswith('<emotion_')]\n    return ' '.join(words)\n\nprint(f'Vocab size: {len(vocab)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:33:52.510335Z","iopub.execute_input":"2025-10-15T19:33:52.510607Z","iopub.status.idle":"2025-10-15T19:34:10.325658Z","shell.execute_reply.started":"2025-10-15T19:33:52.510583Z","shell.execute_reply":"2025-10-15T19:34:10.324850Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 20195\n","output_type":"stream"}],"execution_count":53},{"id":"331a4b21","cell_type":"code","source":"def format_input(row):\n    return f\"Emotion: {row['emotion']} | Situation: {row['Situation']} | Customer: {row['empathetic_dialogues']} Agent:\"\n\ndef format_target(row):\n    return row['labels']\n\nclass ChatDataset(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        x = encode(format_input(row))\n        y = encode(format_target(row))\n        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n\ndef collate_fn(batch):\n    xs, ys = zip(*batch)\n    xs_p = pad_sequence(xs, batch_first=True, padding_value=vocab['<pad>'])\n    ys_p = pad_sequence(ys, batch_first=True, padding_value=vocab['<pad>'])\n    return xs_p, ys_p\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(ChatDataset(train_df), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(ChatDataset(val_df), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(ChatDataset(test_df), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.326471Z","iopub.execute_input":"2025-10-15T19:34:10.327064Z","iopub.status.idle":"2025-10-15T19:34:10.367635Z","shell.execute_reply.started":"2025-10-15T19:34:10.327037Z","shell.execute_reply":"2025-10-15T19:34:10.367048Z"}},"outputs":[],"execution_count":54},{"id":"b5c1395b","cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, q, k, v, mask=None):\n        B = q.size(0)\n        q = self.q_linear(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        k = self.k_linear(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        v = self.v_linear(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-1e9'))\n        attn = torch.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        \n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n        return self.out(out)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        x = self.norm1(x + self.dropout(self.attn(x, x, x, mask)))\n        x = self.norm2(x + self.dropout(self.ff(x)))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_out, enc_out, src_mask)))\n        x = self.norm3(x + self.dropout(self.ff(x)))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, d_model=512, num_heads=2, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n        self.out = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n    \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        src = self.pos_enc(self.embedding(src) * math.sqrt(self.d_model))\n        tgt = self.pos_enc(self.embedding(tgt) * math.sqrt(self.d_model))\n        \n        for layer in self.encoder_layers:\n            src = layer(src, src_mask)\n        \n        for layer in self.decoder_layers:\n            tgt = layer(tgt, src, src_mask, tgt_mask)\n        \n        return self.out(tgt)\n\nmodel = Transformer(len(vocab), d_model=512, num_heads=2, num_layers=2, dropout=0.1).to(device)\nprint(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.369186Z","iopub.execute_input":"2025-10-15T19:34:10.369393Z","iopub.status.idle":"2025-10-15T19:34:10.746089Z","shell.execute_reply.started":"2025-10-15T19:34:10.369377Z","shell.execute_reply":"2025-10-15T19:34:10.745435Z"}},"outputs":[{"name":"stdout","text":"Model parameters: 35,412,707\n","output_type":"stream"}],"execution_count":55},{"id":"7a6348ef","cell_type":"code","source":"def make_causal_mask(size):\n    mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n    return mask\n\ndef make_padding_mask(seq, pad_idx):\n    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\noptimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.746760Z","iopub.execute_input":"2025-10-15T19:34:10.747260Z","iopub.status.idle":"2025-10-15T19:34:10.753438Z","shell.execute_reply.started":"2025-10-15T19:34:10.747242Z","shell.execute_reply":"2025-10-15T19:34:10.752901Z"}},"outputs":[],"execution_count":56},{"id":"eaae726c","cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input = tgt[:, :-1]\n        tgt_target = tgt[:, 1:]\n        \n        tgt_mask = make_causal_mask(tgt_input.size(1)).to(device)\n        \n        optimizer.zero_grad()\n        output = model(src, tgt_input, tgt_mask=tgt_mask)\n        \n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_target.reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.754408Z","iopub.execute_input":"2025-10-15T19:34:10.754638Z","iopub.status.idle":"2025-10-15T19:34:10.769260Z","shell.execute_reply.started":"2025-10-15T19:34:10.754623Z","shell.execute_reply":"2025-10-15T19:34:10.768523Z"}},"outputs":[],"execution_count":57},{"id":"9ab74ab3","cell_type":"code","source":"def greedy_decode(model, src, max_len=50):\n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc = model.pos_enc(model.embedding(src) * math.sqrt(model.d_model))\n        for layer in model.encoder_layers:\n            enc = layer(enc)\n        \n        ys = torch.tensor([[vocab['<bos>']]], device=device)\n        for _ in range(max_len):\n            tgt_mask = make_causal_mask(ys.size(1)).to(device)\n            tgt_emb = model.pos_enc(model.embedding(ys) * math.sqrt(model.d_model))\n            \n            for layer in model.decoder_layers:\n                tgt_emb = layer(tgt_emb, enc, tgt_mask=tgt_mask)\n            \n            logits = model.out(tgt_emb[:, -1, :])\n            next_token = logits.argmax(dim=-1).unsqueeze(0)\n            ys = torch.cat([ys, next_token], dim=1)\n            \n            if next_token.item() == vocab['<eos>']:\n                break\n    \n    return ys.squeeze(0).tolist()\n\ndef beam_search(model, src, beam_width=3, max_len=50):\n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc = model.pos_enc(model.embedding(src) * math.sqrt(model.d_model))\n        for layer in model.encoder_layers:\n            enc = layer(enc)\n        \n        beams = [(torch.tensor([[vocab['<bos>']]], device=device), 0.0)]\n        \n        for _ in range(max_len):\n            new_beams = []\n            for seq, score in beams:\n                if seq[0, -1].item() == vocab['<eos>']:\n                    new_beams.append((seq, score))\n                    continue\n                \n                tgt_mask = make_causal_mask(seq.size(1)).to(device)\n                tgt_emb = model.pos_enc(model.embedding(seq) * math.sqrt(model.d_model))\n                \n                for layer in model.decoder_layers:\n                    tgt_emb = layer(tgt_emb, enc, tgt_mask=tgt_mask)\n                \n                logits = model.out(tgt_emb[:, -1, :])\n                log_probs = torch.log_softmax(logits, dim=-1)\n                top_probs, top_indices = log_probs.topk(beam_width)\n                \n                for i in range(beam_width):\n                    next_token = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n                    next_score = score + top_probs[0, i].item()\n                    next_seq = torch.cat([seq, next_token], dim=1)\n                    new_beams.append((next_seq, next_score))\n            \n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            \n            if all(seq[0, -1].item() == vocab['<eos>'] for seq, _ in beams):\n                break\n    \n    return beams[0][0].squeeze(0).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.770066Z","iopub.execute_input":"2025-10-15T19:34:10.770293Z","iopub.status.idle":"2025-10-15T19:34:10.789105Z","shell.execute_reply.started":"2025-10-15T19:34:10.770268Z","shell.execute_reply":"2025-10-15T19:34:10.788370Z"}},"outputs":[],"execution_count":58},{"id":"1d7eb69e","cell_type":"code","source":"!pip install sacrebleu rouge-score -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:10.789830Z","iopub.execute_input":"2025-10-15T19:34:10.790074Z","iopub.status.idle":"2025-10-15T19:34:14.017758Z","shell.execute_reply.started":"2025-10-15T19:34:10.790058Z","shell.execute_reply":"2025-10-15T19:34:14.016705Z"}},"outputs":[],"execution_count":59},{"id":"82c1897e","cell_type":"code","source":"import sacrebleu\nfrom rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\ndef evaluate(model, loader, max_samples=None, decode_method='greedy', beam_width=3):\n    model.eval()\n    refs, hyps = [], []\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for i, (src, tgt) in enumerate(loader):\n            if max_samples is not None and i * BATCH_SIZE >= max_samples:\n                break\n            \n            src, tgt = src.to(device), tgt.to(device)\n            \n            for b in range(src.size(0)):\n                if decode_method == 'beam':\n                    pred_ids = beam_search(model, src[b:b+1], beam_width=beam_width, max_len=50)\n                else:\n                    pred_ids = greedy_decode(model, src[b:b+1], max_len=50)\n                \n                ref_ids = tgt[b].cpu().tolist()\n                \n                pred_text = decode(pred_ids)\n                ref_text = decode(ref_ids)\n                \n                hyps.append(pred_text)\n                refs.append(ref_text)\n            \n            tgt_input = tgt[:, :-1]\n            tgt_target = tgt[:, 1:]\n            tgt_mask = make_causal_mask(tgt_input.size(1)).to(device)\n            output = model(src, tgt_input, tgt_mask=tgt_mask)\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_target.reshape(-1))\n            \n            non_pad = (tgt_target != vocab['<pad>']).sum().item()\n            total_loss += loss.item() * non_pad\n            total_tokens += non_pad\n    \n    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n    rouge_l = sum(scorer.score(r, h)['rougeL'].fmeasure for r, h in zip(refs, hyps)) / len(refs) * 100\n    chrf = sacrebleu.corpus_chrf(hyps, [refs])\n    ppl = math.exp(total_loss / total_tokens) if total_tokens > 0 else float('inf')\n    \n    return {'bleu': bleu.score, 'rouge_l': rouge_l, 'chrf': chrf.score, 'ppl': ppl, 'samples': list(zip(refs[:5], hyps[:5]))}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:14.019135Z","iopub.execute_input":"2025-10-15T19:34:14.019566Z","iopub.status.idle":"2025-10-15T19:34:14.030562Z","shell.execute_reply.started":"2025-10-15T19:34:14.019514Z","shell.execute_reply":"2025-10-15T19:34:14.029766Z"}},"outputs":[],"execution_count":60},{"id":"3badc20f","cell_type":"code","source":"EPOCHS = 20\nbest_bleu = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n    metrics = evaluate(model, val_loader, max_samples=500, decode_method='greedy')\n    \n    print(f\"Epoch {epoch}/{EPOCHS}\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Val BLEU: {metrics['bleu']:.2f}, ROUGE-L: {metrics['rouge_l']:.2f}, chrF: {metrics['chrf']:.2f}, PPL: {metrics['ppl']:.2f}\")\n    \n    if metrics['bleu'] > best_bleu:\n        best_bleu = metrics['bleu']\n        torch.save({\n            'model': model.state_dict(),\n            'vocab': vocab,\n            'inv_vocab': inv_vocab,\n            'metrics': metrics\n        }, 'best_model.pt')\n        print(f\"  *** Saved best model (BLEU: {best_bleu:.2f}) ***\")\n    \n    if epoch % 2 == 0:\n        print(\"\\n  Sample predictions:\")\n        for ref, hyp in metrics['samples'][:3]:\n            print(f\"    Ref: {ref}\")\n            print(f\"    Hyp: {hyp}\")\n            print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:34:14.031296Z","iopub.execute_input":"2025-10-15T19:34:14.031481Z","iopub.status.idle":"2025-10-15T20:24:26.307174Z","shell.execute_reply.started":"2025-10-15T19:34:14.031465Z","shell.execute_reply":"2025-10-15T20:24:26.306351Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n  Train Loss: 4.9213\n  Val BLEU: 1.21, ROUGE-L: 12.93, chrF: 12.43, PPL: 82.23\n  *** Saved best model (BLEU: 1.21) ***\nEpoch 2/20\n  Train Loss: 4.2549\n  Val BLEU: 1.46, ROUGE-L: 12.82, chrF: 11.56, PPL: 69.29\n  *** Saved best model (BLEU: 1.46) ***\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i am not sure . i am so i am so i am so sorry .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: i 'm sure you will be a lot of people .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i was a lot of people who did n't get it .\n\nEpoch 3/20\n  Train Loss: 4.0822\n  Val BLEU: 1.65, ROUGE-L: 14.41, chrF: 12.05, PPL: 63.58\n  *** Saved best model (BLEU: 1.65) ***\nEpoch 4/20\n  Train Loss: 3.9645\n  Val BLEU: 1.70, ROUGE-L: 14.55, chrF: 11.80, PPL: 59.42\n  *** Saved best model (BLEU: 1.70) ***\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i am so happy for you .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: i 'm sure it was a good thing .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't have to be able to get it .\n\nEpoch 5/20\n  Train Loss: 3.8700\n  Val BLEU: 1.87, ROUGE-L: 14.46, chrF: 12.21, PPL: 56.03\n  *** Saved best model (BLEU: 1.87) ***\nEpoch 6/20\n  Train Loss: 3.7894\n  Val BLEU: 2.13, ROUGE-L: 14.28, chrF: 12.22, PPL: 54.55\n  *** Saved best model (BLEU: 2.13) ***\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i am so happy for you .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: i 'm sure it will be a great time .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i am not sure . i am going to be a lot of people .\n\nEpoch 7/20\n  Train Loss: 3.7188\n  Val BLEU: 2.39, ROUGE-L: 14.58, chrF: 12.95, PPL: 52.76\n  *** Saved best model (BLEU: 2.39) ***\nEpoch 8/20\n  Train Loss: 3.6536\n  Val BLEU: 1.97, ROUGE-L: 14.71, chrF: 12.69, PPL: 52.17\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i 'm not sure yet . i 'm a little bit of my wife and i 'm not sure how to be .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: i 'm sure it will be a great time .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't know what i was wrong with my job .\n\nEpoch 9/20\n  Train Loss: 3.5918\n  Val BLEU: 2.23, ROUGE-L: 15.49, chrF: 12.27, PPL: 51.46\nEpoch 10/20\n  Train Loss: 3.5311\n  Val BLEU: 2.34, ROUGE-L: 15.43, chrF: 12.49, PPL: 52.05\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i 'm not sure yet . i 'm just happy for her .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: that 's great . i hope you guys have a good time .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't know what i was wrong with my friends .\n\nEpoch 11/20\n  Train Loss: 3.4738\n  Val BLEU: 2.00, ROUGE-L: 14.60, chrF: 12.60, PPL: 50.29\nEpoch 12/20\n  Train Loss: 3.4155\n  Val BLEU: 2.16, ROUGE-L: 14.93, chrF: 12.82, PPL: 50.68\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i 'm a little bit of my kids . i 'm so proud of her .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: that 's great ! i hope you have a great time !\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't even know what i mean , but i 'm not sure i 'm going to do .\n\nEpoch 13/20\n  Train Loss: 3.3559\n  Val BLEU: 2.41, ROUGE-L: 15.43, chrF: 12.64, PPL: 51.03\n  *** Saved best model (BLEU: 2.41) ***\nEpoch 14/20\n  Train Loss: 3.2998\n  Val BLEU: 2.44, ROUGE-L: 14.83, chrF: 13.82, PPL: 51.21\n  *** Saved best model (BLEU: 2.44) ***\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i 'm a little proud . i 'm so proud of her .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: that 's great ! i 'm sure it will be a great feeling .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't know what i was wrong with my friends . i got a girl and i was pretty happy .\n\nEpoch 15/20\n  Train Loss: 3.2432\n  Val BLEU: 2.12, ROUGE-L: 14.54, chrF: 13.56, PPL: 51.90\nEpoch 16/20\n  Train Loss: 3.1860\n  Val BLEU: 2.16, ROUGE-L: 14.63, chrF: 12.83, PPL: 53.13\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i 'm not sure , but i 'm just happy for her .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: oh , i 'm so happy for you !\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i did n't know what to do . i 'm going to do .\n\nEpoch 17/20\n  Train Loss: 3.1286\n  Val BLEU: 2.27, ROUGE-L: 14.49, chrF: 13.41, PPL: 53.62\nEpoch 18/20\n  Train Loss: 3.0717\n  Val BLEU: 2.24, ROUGE-L: 15.25, chrF: 13.57, PPL: 54.56\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: i have two girls and they are 8 years old .\n\n    Ref: that 's sounds nice and romantic\n    Hyp: oh wow , that 's pretty cool . i hope it goes well .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i think i should be able to get it back on my first time .\n\nEpoch 19/20\n  Train Loss: 3.0126\n  Val BLEU: 2.34, ROUGE-L: 14.58, chrF: 13.18, PPL: 55.87\nEpoch 20/20\n  Train Loss: 2.9558\n  Val BLEU: 1.84, ROUGE-L: 14.12, chrF: 12.13, PPL: 57.88\n\n  Sample predictions:\n    Ref: 10 and 13\n    Hyp: 4\n\n    Ref: that 's sounds nice and romantic\n    Hyp: wow , that 's amazing ! i love my husband .\n\n    Ref: because girls find it more attractive . in my experience\n    Hyp: i think i would like to say the best if i 'm not sure i think i 'm just bought a girl i 'm not sure i think i 'm gon na miss her .\n\n","output_type":"stream"}],"execution_count":61},{"id":"8c3b1cdf","cell_type":"code","source":"checkpoint = torch.load('/kaggle/working/best_model.pt')\nmodel.load_state_dict(checkpoint['model'])\n\nprint(\"=== GREEDY DECODING ===\")\ntest_metrics_greedy = evaluate(model, test_loader, max_samples=1000, decode_method='greedy')\nprint(f\"BLEU: {test_metrics_greedy['bleu']:.2f}\")\nprint(f\"ROUGE-L: {test_metrics_greedy['rouge_l']:.2f}\")\nprint(f\"chrF: {test_metrics_greedy['chrf']:.2f}\")\nprint(f\"Perplexity: {test_metrics_greedy['ppl']:.2f}\")\n\nprint(\"\\n=== BEAM SEARCH (width=3) ===\")\ntest_metrics_beam = evaluate(model, test_loader, max_samples=None, decode_method='beam', beam_width=3)\nprint(f\"BLEU: {test_metrics_beam['bleu']:.2f}\")\nprint(f\"ROUGE-L: {test_metrics_beam['rouge_l']:.2f}\")\nprint(f\"chrF: {test_metrics_beam['chrf']:.2f}\")\nprint(f\"Perplexity: {test_metrics_beam['ppl']:.2f}\")\n\nprint(\"\\n=== GREEDY Sample Outputs ===\")\nfor i, (ref, hyp) in enumerate(test_metrics_greedy['samples'], 1):\n    print(f\"\\nExample {i}:\")\n    print(f\"Reference: {ref}\")\n    print(f\"Generated: {hyp}\")\n\nprint(\"\\n=== BEAM SEARCH Sample Outputs ===\")\nfor i, (ref, hyp) in enumerate(test_metrics_beam['samples'], 1):\n    print(f\"\\nExample {i}:\")\n    print(f\"Reference: {ref}\")\n    print(f\"Generated: {hyp}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:24:26.308080Z","iopub.execute_input":"2025-10-15T20:24:26.308623Z","iopub.status.idle":"2025-10-15T20:36:59.671793Z","shell.execute_reply.started":"2025-10-15T20:24:26.308598Z","shell.execute_reply":"2025-10-15T20:36:59.671035Z"}},"outputs":[{"name":"stdout","text":"=== GREEDY DECODING ===\nBLEU: 2.48\nROUGE-L: 15.01\nchrF: 13.47\nPerplexity: 49.25\n\n=== BEAM SEARCH (width=3) ===\nBLEU: 2.05\nROUGE-L: 14.44\nchrF: 11.14\nPerplexity: 49.07\n\n=== GREEDY Sample Outputs ===\n\nExample 1:\nReference: well have you been living a happy life ? she would probably be happy about that .\nGenerated: i 'm sorry to hear that . i hope you have a good memories of her .\n\nExample 2:\nReference: ah man ! you should n't feel bad about tripping . why do you feel bad about it ?\nGenerated: oh no ! did you hurt yourself ?\n\nExample 3:\nReference: it is , and yeah until they are over the age of you go go every other month crazy poor babies ! !\nGenerated: she was a very good idea .\n\nExample 4:\nReference: awww ... still thats upsetting\nGenerated: i 'm sorry to hear that . what 's the job is it ?\n\nExample 5:\nReference: yes , but it is crazy expensive to go .\nGenerated: i 'm going to buy a lot of money .\n\n=== BEAM SEARCH Sample Outputs ===\n\nExample 1:\nReference: well have you been living a happy life ? she would probably be happy about that .\nGenerated: i 'm sorry to hear that . i hope you find someone better .\n\nExample 2:\nReference: ah man ! you should n't feel bad about tripping . why do you feel bad about it ?\nGenerated: oh no ! did you get hurt ?\n\nExample 3:\nReference: it is , and yeah until they are over the age of you go go every other month crazy poor babies ! !\nGenerated: yes , she had a lot of fun .\n\nExample 4:\nReference: awww ... still thats upsetting\nGenerated: i 'm sorry to hear that . are you going to be a good job ?\n\nExample 5:\nReference: yes , but it is crazy expensive to go .\nGenerated: i am going to take a trip .\n","output_type":"stream"}],"execution_count":62},{"id":"de7be1b7","cell_type":"code","source":"!pip install streamlit -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:36:59.673722Z","iopub.execute_input":"2025-10-15T20:36:59.673921Z","iopub.status.idle":"2025-10-15T20:37:02.893418Z","shell.execute_reply.started":"2025-10-15T20:36:59.673906Z","shell.execute_reply":"2025-10-15T20:37:02.892518Z"}},"outputs":[],"execution_count":63},{"id":"97286088","cell_type":"code","source":"app_code = '''import streamlit as st\nimport torch\nimport torch.nn as nn\nimport math\nfrom nltk.tokenize import word_tokenize\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\\'pe\\', pe.unsqueeze(0))\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, q, k, v, mask=None):\n        B = q.size(0)\n        q = self.q_linear(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        k = self.k_linear(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        v = self.v_linear(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float(\\'-1e9\\'))\n        attn = torch.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n        return self.out(out), attn\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n    def forward(self, x):\n        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, mask=None):\n        attn_out, _ = self.attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        x = self.norm2(x + self.dropout(self.ff(x)))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n        self_attn_out, self_attn_weights = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(self_attn_out))\n        cross_attn_out, cross_attn_weights = self.cross_attn(x, enc_out, enc_out, src_mask)\n        x = self.norm2(x + self.dropout(cross_attn_out))\n        x = self.norm3(x + self.dropout(self.ff(x)))\n        return x, cross_attn_weights\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, d_model=512, num_heads=2, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n        self.out = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        src = self.pos_enc(self.embedding(src) * math.sqrt(self.d_model))\n        tgt = self.pos_enc(self.embedding(tgt) * math.sqrt(self.d_model))\n        for layer in self.encoder_layers:\n            src = layer(src, src_mask)\n        for layer in self.decoder_layers:\n            tgt, _ = layer(tgt, src, src_mask, tgt_mask)\n        return self.out(tgt)\n\n@st.cache_resource\ndef load_model():\n    checkpoint = torch.load(\\'best_model.pt\\', map_location=\\'cpu\\')\n    vocab = checkpoint[\\'vocab\\']\n    inv_vocab = checkpoint[\\'inv_vocab\\']\n    model = Transformer(len(vocab))\n    model.load_state_dict(checkpoint[\\'model\\'])\n    model.eval()\n    return model, vocab, inv_vocab\n\ndef encode(text, vocab):\n    tokens = [\\'<bos>\\'] + word_tokenize(text.lower()) + [\\'<eos>\\']\n    return [vocab.get(t, vocab[\\'<unk>\\']) for t in tokens]\n\ndef decode(ids, inv_vocab):\n    words = [inv_vocab.get(i, \\'<unk>\\') for i in ids]\n    words = [w for w in words if w not in [\\'<pad>\\', \\'<bos>\\', \\'<eos>\\', \\'<unk>\\', \\'<sep>\\'] and not w.startswith(\\'<emotion_\\')]\n    return \\' \\'.join(words)\n\ndef make_causal_mask(size):\n    return torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n\ndef generate(model, src_text, vocab, inv_vocab, method=\\'greedy\\', beam_width=3, max_len=50):\n    src_ids = torch.tensor([encode(src_text, vocab)])\n    \n    with torch.no_grad():\n        enc = model.pos_enc(model.embedding(src_ids) * math.sqrt(model.d_model))\n        for layer in model.encoder_layers:\n            enc = layer(enc)\n        \n        if method == \\'greedy\\':\n            ys = torch.tensor([[vocab[\\'<bos>\\']]])\n            for _ in range(max_len):\n                tgt_mask = make_causal_mask(ys.size(1))\n                tgt_emb = model.pos_enc(model.embedding(ys) * math.sqrt(model.d_model))\n                for layer in model.decoder_layers:\n                    tgt_emb, _ = layer(tgt_emb, enc, tgt_mask=tgt_mask)\n                logits = model.out(tgt_emb[:, -1, :])\n                next_token = logits.argmax(dim=-1).unsqueeze(0)\n                ys = torch.cat([ys, next_token], dim=1)\n                if next_token.item() == vocab[\\'<eos>\\']:\n                    break\n            return decode(ys.squeeze(0).tolist(), inv_vocab)\n        \n        else:\n            beams = [(torch.tensor([[vocab[\\'<bos>\\']]]), 0.0)]\n            for _ in range(max_len):\n                new_beams = []\n                for seq, score in beams:\n                    if seq[0, -1].item() == vocab[\\'<eos>\\']:\n                        new_beams.append((seq, score))\n                        continue\n                    tgt_mask = make_causal_mask(seq.size(1))\n                    tgt_emb = model.pos_enc(model.embedding(seq) * math.sqrt(model.d_model))\n                    for layer in model.decoder_layers:\n                        tgt_emb, _ = layer(tgt_emb, enc, tgt_mask=tgt_mask)\n                    logits = model.out(tgt_emb[:, -1, :])\n                    log_probs = torch.log_softmax(logits, dim=-1)\n                    top_probs, top_indices = log_probs.topk(beam_width)\n                    for i in range(beam_width):\n                        next_token = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n                        next_score = score + top_probs[0, i].item()\n                        next_seq = torch.cat([seq, next_token], dim=1)\n                        new_beams.append((next_seq, next_score))\n                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n                if all(seq[0, -1].item() == vocab[\\'<eos>\\'] for seq, _ in beams):\n                    break\n            return decode(beams[0][0].squeeze(0).tolist(), inv_vocab)\n\nst.title(\"ðŸ¤– Empathetic Chatbot\")\nst.markdown(\"Transformer-based empathetic conversational agent\")\n\nmodel, vocab, inv_vocab = load_model()\n\nif \\'history\\' not in st.session_state:\n    st.session_state.history = []\n\nwith st.sidebar:\n    st.header(\"Settings\")\n    emotion = st.selectbox(\"Emotion\", [\\'afraid\\', \\'angry\\', \\'annoyed\\', \\'anxious\\', \\'sad\\', \\'happy\\', \\'excited\\', \\'grateful\\', \\'proud\\', \\'surprised\\'])\n    situation = st.text_area(\"Situation (optional)\", \"\")\n    method = st.radio(\"Decoding\", [\\'greedy\\', \\'beam\\'])\n    if st.button(\"Clear History\"):\n        st.session_state.history = []\n\nuser_input = st.chat_input(\"Type your message...\")\n\nif user_input:\n    sit = situation if situation else \"general conversation\"\n    input_text = f\"Emotion: {emotion} | Situation: {sit} | Customer: {user_input} Agent:\"\n    \n    with st.spinner(\"Thinking...\"):\n        response = generate(model, input_text, vocab, inv_vocab, method=method)\n    \n    st.session_state.history.append((\\'user\\', user_input))\n    st.session_state.history.append((\\'bot\\', response))\n\nfor role, msg in st.session_state.history:\n    with st.chat_message(role):\n        st.write(msg)\n'''\n\nwith open('app.py', 'w', encoding='utf-8') as f:\n    f.write(app_code)\n\nprint(\"Streamlit app saved to app.py\")\nprint(\"Run with: streamlit run app.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:37:02.894667Z","iopub.execute_input":"2025-10-15T20:37:02.894974Z","iopub.status.idle":"2025-10-15T20:37:02.905952Z","shell.execute_reply.started":"2025-10-15T20:37:02.894943Z","shell.execute_reply":"2025-10-15T20:37:02.905335Z"}},"outputs":[{"name":"stdout","text":"Streamlit app saved to app.py\nRun with: streamlit run app.py\n","output_type":"stream"}],"execution_count":64},{"id":"82079562","cell_type":"code","source":"report = f'''# Empathetic Chatbot Evaluation Report\n\n## Model Architecture\n- Transformer encoder-decoder (built from scratch)\n- Embedding dimension: 512\n- Attention heads: 2\n- Encoder/Decoder layers: 2 each\n- Dropout: 0.1\n- Vocabulary size: {len(vocab)}\n- Total parameters: {sum(p.numel() for p in model.parameters()):,}\n\n## Dataset Split\n- Train: {len(train_df)} samples (80%)\n- Validation: {len(val_df)} samples (10%)\n- Test: {len(test_df)} samples (10%)\n\n## Training Configuration\n- Optimizer: Adam (lr=1e-4, betas=(0.9, 0.98))\n- Batch size: 64\n- Loss: CrossEntropyLoss (ignore padding)\n- Teacher forcing: Yes\n- Epochs: {EPOCHS}\n- Best model selection: Validation BLEU\n\n## Test Set Results (Full Dataset)\n\n### Greedy Decoding\n- BLEU: {test_metrics_greedy[\"bleu\"]:.2f}\n- ROUGE-L: {test_metrics_greedy[\"rouge_l\"]:.2f}\n- chrF: {test_metrics_greedy[\"chrf\"]:.2f}\n- Perplexity: {test_metrics_greedy[\"ppl\"]:.2f}\n\n### Beam Search (width=3)\n- BLEU: {test_metrics_beam[\"bleu\"]:.2f}\n- ROUGE-L: {test_metrics_beam[\"rouge_l\"]:.2f}\n- chrF: {test_metrics_beam[\"chrf\"]:.2f}\n- Perplexity: {test_metrics_beam[\"ppl\"]:.2f}\n\n## Qualitative Examples (Greedy)\n'''\n\nfor i, (ref, hyp) in enumerate(test_metrics_greedy['samples'], 1):\n    report += f'''\\n### Example {i}\n**Reference:** {ref}\n**Generated:** {hyp}\n'''\n\nreport += '\\n## Qualitative Examples (Beam Search)\\n'\n\nfor i, (ref, hyp) in enumerate(test_metrics_beam['samples'], 1):\n    report += f'''\\n### Example {i}\n**Reference:** {ref}\n**Generated:** {hyp}\n'''\n\nreport += '''\\n## Implementation Details\n- Multi-head attention with residual connections and layer normalization\n- Sinusoidal positional encoding\n- Causal masking in decoder self-attention\n- Greedy and beam search decoding (beam width=3)\n- Special tokens: <pad>, <bos>, <eos>, <unk>, <sep>, <emotion_X>\n\n## Deployment\n- Framework: Streamlit\n- Features: Interactive chat, emotion selection, conversation history, decoding method selection\n- Run: `streamlit run app.py`\n'''\n\nwith open('/kaggle/working/EVALUATION_REPORT.md', 'w', encoding='utf-8') as f:\n    f.write(report)\n\nprint(\"Evaluation report saved to /kaggle/working/EVALUATION_REPORT.md\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:37:02.906800Z","iopub.execute_input":"2025-10-15T20:37:02.907051Z","iopub.status.idle":"2025-10-15T20:37:02.927282Z","shell.execute_reply.started":"2025-10-15T20:37:02.907030Z","shell.execute_reply":"2025-10-15T20:37:02.926719Z"}},"outputs":[{"name":"stdout","text":"Evaluation report saved to /kaggle/working/EVALUATION_REPORT.md\n","output_type":"stream"}],"execution_count":65},{"id":"ae593d87","cell_type":"code","source":"readme = '''# Empathetic Conversational Chatbot\n\nTransformer encoder-decoder model for empathetic dialogue generation.\n\n## Setup\n```bash\npip install torch pandas numpy nltk scikit-learn sacrebleu rouge-score streamlit\n```\n\n## Training\nRun all cells in `project2_complete.ipynb`\n\n## Inference\n```bash\nstreamlit run app.py\n```\n\n## Files\n- `project2_complete.ipynb`: Complete training pipeline\n- `app.py`: Streamlit chatbot interface\n- `best_model.pt`: Trained model checkpoint\n- `EVALUATION_REPORT.md`: Metrics and analysis\n\n## Model Architecture\n- Transformer encoder-decoder (from scratch)\n- 512-dim embeddings, 2 heads, 2 layers\n- Positional encoding, multi-head attention, residual connections\n- Teacher forcing during training\n- Greedy and beam search decoding\n\n## Dataset\nEmpathetic Dialogues (Kaggle)\n- Input: Emotion + Situation + Customer utterance\n- Output: Agent empathetic reply\n- Split: 80/10/10 train/val/test\n\n## Results\nSee `EVALUATION_REPORT.md` for detailed metrics and examples.\n'''\n\nwith open('README.md', 'w', encoding='utf-8') as f:\n    f.write(readme)\n\nprint(\"README.md created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:37:02.927888Z","iopub.execute_input":"2025-10-15T20:37:02.928051Z","iopub.status.idle":"2025-10-15T20:37:02.945144Z","shell.execute_reply.started":"2025-10-15T20:37:02.928038Z","shell.execute_reply":"2025-10-15T20:37:02.944533Z"}},"outputs":[{"name":"stdout","text":"README.md created\n","output_type":"stream"}],"execution_count":66}]}